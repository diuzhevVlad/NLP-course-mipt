{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Важно!** \n",
    "\n",
    "Домашнее задание состоит из нескольких задач, которые вам нужно решить.\n",
    "*   Баллы выставляются по принципу выполнено/невыполнено.\n",
    "*   За каждую выполненую задачу вы получаете баллы (количество баллов за задание указано в скобках).\n",
    "\n",
    "**Инструкция выполнения:** Выполните задания в этом же ноутбуке (места под решения **КАЖДОЙ** задачи обозначены как **#НАЧАЛО ВАШЕГО РЕШЕНИЯ** и **#КОНЕЦ ВАШЕГО РЕШЕНИЯ**)\n",
    "\n",
    "**Как отправить задание на проверку:** Вам необходимо сохранить ваше решение в данном блокноте и отправить итоговый **файл .IPYNB** в личном сообщении Telegram.\n",
    "\n",
    "# **Прежде чем проверять задания:**\n",
    "\n",
    "1. Перезапустите **ядро (restart the kernel)**: в меню, выбрать **Ядро (Kernel)**\n",
    "→ **Перезапустить (Restart)**\n",
    "2. Затем **Выполнить** **все ячейки (run all cells)**: в меню, выбрать **Ячейка (Cell)**\n",
    "→ **Запустить все (Run All)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение в Parameter Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "В данном задании основное внимание уделяется изучению и применению современных методов **Parameter Efficient Fine-Tuning (PEFT)**. Эти подходы позволяют эффективно дообучать большие языковые модели, используя лишь небольшую часть параметров, что критически важно при работе с ограниченными вычислительными ресурсами.\n",
    "\n",
    "### Обязательные PEFT методы для изучения:\n",
    "\n",
    "1. **LoRA (Low-Rank Adaptation)**\n",
    "   - Разложение весовых матриц на произведение матриц низкого ранга\n",
    "   - Ключевые гиперпараметры: `r` (rank), `alpha`, `dropout`, `target_modules`\n",
    "\n",
    "2. **QLoRA (Quantized LoRA)** \n",
    "   - Комбинация 4-bit квантизации (NF4) с LoRA\n",
    "   - Значительно снижает потребление GPU памяти\n",
    "\n",
    "3. **AdaLoRA (Adaptive LoRA)**\n",
    "   - Динамическое изменение ранга во время обучения\n",
    "   - Автоматическая оптимизация распределения параметров\n",
    "\n",
    "### Дополнительные методы (по выбору):\n",
    "- **IA³ (Infused Adapter by Inhibiting and Amplifying Inner Activations)**\n",
    "- **Prefix Tuning / P-Tuning v2**\n",
    "- **Prompt Tuning**\n",
    "\n",
    "### Методология сравнения PEFT подходов:\n",
    "\n",
    "Для каждого метода необходимо измерить и сравнить:\n",
    "\n",
    "#### Эффективность ресурсов:\n",
    "- **Количество обучаемых параметров** (в % от общего числа параметров модели)\n",
    "- **Потребление GPU памяти** (в GB во время обучения и инференса)\n",
    "- **Время обучения** (сек/эпоху)\n",
    "- **Скорость инференса** (токенов/секунду)\n",
    "\n",
    "#### Качество результатов:\n",
    "- **Основные метрики** в зависимости от задачи (ROUGE для суммаризации, BLEU для перевода)\n",
    "- **Стабильность обучения** (сходимость функции потерь)\n",
    "- **Качественный анализ** выходных текстов\n",
    "\n",
    "#### Требования к отчету:\n",
    "1. **Сравнительная таблица** всех протестированных методов\n",
    "2. **Графики Pareto-frontier**: эффективность vs качество\n",
    "3. **Обоснованные рекомендации** по выбору метода для различных сценариев\n",
    "4. **Анализ компромиссов** между точностью и эффективностью\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1: Parameter Efficient Fine-Tuning (PEFT) моделей для суммаризации текстов\n",
    "\n",
    "Цель задания — научиться применять современные методы параметрически-эффективного дообучения (PEFT) для настройки больших языковых моделей на задачу суммаризации текстов. Вы реализуете различные PEFT подходы: LoRA, QLoRA из библиотеки PEFT.\n",
    "\n",
    "#### Задачи:\n",
    "\n",
    "1. **Выбор датасета**:\n",
    "   - Загрузите датасет для задачи суммаризации, например, датасет `CNN/DailyMail`, содержащий новостные статьи и их краткие содержания (референсы). Используйте библиотеку `datasets` для загрузки данных.\n",
    "   - Обратите внимание на то, что вы можете использовать и другие подходящие датасеты для суммаризации (например, XSum).\n",
    "\n",
    "2. **Предобработка данных**:\n",
    "   - **Разделите** данные на обучающую и тестовую выборки.\n",
    "   - **Очистите текст** от лишних символов, специальных токенов и пробелов.\n",
    "   - **Подготовьте данные** в формате, подходящем для выбранной модели:\n",
    "     - Для GPT-2 вам нужно будет подать текст целиком (входной текст + референсное суммирование в одном формате).\n",
    "     - Для T5 модель требует форматировать входные данные в виде `summarize: <текст>` для текстов, которые нужно суммировать.\n",
    "\n",
    "3. **Создание модели**:\n",
    "   - **GPT-2**:\n",
    "     - Импортируйте предобученную модель `GPT2LMHeadModel` из библиотеки Hugging Face.\n",
    "     - Модель GPT-2 изначально не предобучена для задачи суммаризации, поэтому требуется её дообучение на подходящих данных.\n",
    "     - Поддержите правильное управление длиной сгенерированного текста при инференсе, чтобы обеспечить краткость суммаризаций.\n",
    "   \n",
    "   - **T5**:\n",
    "     - Для T5 используйте модель `T5ForConditionalGeneration`. T5 уже предобучена на множестве задач, включая суммаризацию, поэтому она лучше подходит для данной задачи.\n",
    "     - В отличие от GPT-2, модель T5 обучена на задаче, где входной текст — это задание (например, \"summarize:\") + текст для обработки, а выход — краткое содержание. Это нужно учесть при подготовке данных\".\n",
    "     - Импортируйте модель из библиотеки Hugging Face и настройте её для использования на задаче суммаризации.\n",
    "\n",
    "4. **Настройка параметров обучения**:\n",
    "   - Настройте параметры обучения для обеих моделей:\n",
    "     - Количество эпох.\n",
    "     - Размер батча.\n",
    "     - Скорость обучения.\n",
    "     - Выберите оптимизатор (например, AdamW).\n",
    "   - Для T5 используйте кросс-энтропийную функцию потерь (`CrossEntropyLoss`), так как это задача генерации текста с \"условной вероятностью\". Для GPT-2 используйте ту же функцию с учётом автогрегрессивного генерационного процесса.\n",
    "\n",
    "5. **PEFT Fine-tuning модели** (основная часть задания):\n",
    "   - **Обязательно** реализуйте дообучение с использованием различных PEFT методов:\n",
    "     - **LoRA (Low-Rank Adaptation)**: Настройте параметры rank (r), alpha, dropout\n",
    "     - **QLoRA (Quantized LoRA)**: Используйте 4-bit квантизацию с LoRA\n",
    "     - **AdaLoRA**: Адаптивное изменение ранга во время обучения\n",
    "     - **Дополнительно**: попробуйте IA³ (Infused Adapter by Inhibiting and Amplifying Inner Activations) или Prompt Tuning\n",
    "   - Для каждого PEFT метода:\n",
    "     - Настройте специфические гиперпараметры (rank, alpha, target_modules)\n",
    "     - Измерьте количество обучаемых параметров\n",
    "     - Зафиксируйте время обучения и потребление памяти\n",
    "   - Используйте класс `Trainer` или `SFTTrainer` из библиотеки `trl` для процесса дообучения\n",
    "   - Сравните результаты full fine-tuning с PEFT подходами (опционально)\n",
    "\n",
    "6. **Инференс**:\n",
    "   - Используйте обе модели для генерации кратких содержаний на тестовой выборке.\n",
    "   - Подготовьте несколько примеров суммаризаций и выведите результаты для каждой модели.\n",
    "   - Для инференса используйте разные стратегии декодирования:\n",
    "     - **Greedy decoding** (жадный поиск).\n",
    "     - **Beam search** (поиск по нескольким лучам).\n",
    "     - **Sampling** (стохастическая генерация с использованием вероятностей).\n",
    "   - Сравните результаты, чтобы понять, как разные стратегии влияют на качество суммаризаций.\n",
    "\n",
    "7. **Сравнительная оценка PEFT методов**:\n",
    "   - Создайте сравнительную таблицу для всех протестированных методов:\n",
    "     - Количество обучаемых параметров (в % от общего числа параметров модели)\n",
    "     - Время обучения на эпоху\n",
    "     - Потребление GPU памяти\n",
    "     - Качество суммаризации по метрикам ROUGE и BLEU\n",
    "   - Оцените качество суммаризаций с использованием метрик:\n",
    "     - **ROUGE-1, ROUGE-2, ROUGE-L** — для оценки точности и полноты суммаризаций\n",
    "     - **BLEU** — для оценки схожести с референсным текстом\n",
    "   - Проанализируйте trade-off между эффективностью обучения и качеством результата\n",
    "\n",
    "#### Ожидаемые результаты:\n",
    "- **Код с реализацией различных PEFT методов** для выбранной модели (GPT-2 или T5)\n",
    "- **Сравнительный анализ** всех протестированных PEFT подходов с детальными метриками эффективности\n",
    "- **Отчет** с обоснованием выбора оптимального PEFT метода для задачи суммаризации\n",
    "- **Примеры суммаризаций** для каждого PEFT метода с качественным анализом различий\n",
    "- **Рекомендации** по выбору PEFT подхода в зависимости от ограничений по ресурсам\n",
    "\n",
    "#### Рекомендуемые ресурсы:\n",
    "- **[PEFT Documentation](https://huggingface.co/docs/peft/index)** - основная документация библиотеки PEFT\n",
    "- **[LoRA Developer Guide](https://huggingface.co/docs/peft/developer_guides/lora)** - детальное руководство по LoRA\n",
    "- **[QLoRA Implementation](https://huggingface.co/docs/peft/developer_guides/quantization)** - квантизация и QLoRA\n",
    "- [Документация Hugging Face Transformers](https://huggingface.co/docs/transformers/index)\n",
    "- [Документация Hugging Face Datasets](https://huggingface.co/docs/datasets/index)\n",
    "- **[PEFT Examples](https://github.com/huggingface/peft/tree/main/examples)** - примеры использования различных PEFT методов\n",
    "- **[TRL SFTTrainer](https://huggingface.co/docs/trl/sft_trainer)** - для supervised fine-tuning\n",
    "\n",
    "#### Критерии оценки:\n",
    "- **Корректность реализации PEFT методов** (30%) - правильная настройка и применение различных PEFT подходов\n",
    "- **Полнота сравнительного анализа** (25%) - детальное сравнение методов по всем указанным метрикам\n",
    "- **Качество сгенерированных суммаризаций** (20%) - оценка по ROUGE/BLEU метрикам\n",
    "- **Глубина анализа и выводов** (15%) - обоснованные рекомендации по выбору PEFT метода\n",
    "- **Четкость и структурированность отчета** (10%) - качество документации процесса и результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n",
      "GPU: NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 1) Imports & basic setup\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, AdaLoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {DEVICE}\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# 2) Small utility helpers\n",
    "\n",
    "def count_trainable(model: torch.nn.Module) -> Dict[str, Any]:\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return {\n",
    "        \"total\": int(total),\n",
    "        \"trainable\": int(trainable),\n",
    "        \"trainable_pct\": 100.0 * trainable / total,\n",
    "    }\n",
    "\n",
    "\n",
    "def cuda_mem_stats() -> Dict[str, int]:\n",
    "    if not torch.cuda.is_available():\n",
    "        return {\"allocated_bytes\": 0, \"reserved_bytes\": 0}\n",
    "    idx = torch.cuda.current_device()\n",
    "    return {\n",
    "        \"allocated_bytes\": int(torch.cuda.memory_allocated(idx)),\n",
    "        \"max_allocated_bytes\": int(torch.cuda.max_memory_allocated(idx)),\n",
    "        \"reserved_bytes\": int(torch.cuda.memory_reserved(idx)),\n",
    "        \"max_reserved_bytes\": int(torch.cuda.max_memory_reserved(idx)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# 3) Data: load & preprocess (CNN/DailyMail)\n",
    "\n",
    "def load_cnn_dm(sample_train: Optional[int] = None, dataset_config: str = \"3.0.0\"):\n",
    "    ds = load_dataset(\"cnn_dailymail\", dataset_config)\n",
    "    train = ds[\"train\"]\n",
    "    valid = ds[\"validation\"] if \"validation\" in ds else ds[\"test\"]\n",
    "    if sample_train:\n",
    "        train = train.select(range(min(sample_train, len(train))))\n",
    "        valid_n = max(1, sample_train // 10)\n",
    "        valid = valid.select(range(min(valid_n, len(valid))))\n",
    "    return train, valid\n",
    "\n",
    "\n",
    "def _strip_fields(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    for k in (\"article\", \"highlights\", \"summary\", \"document\", \"text\"):\n",
    "        if k in example and isinstance(example[k], str):\n",
    "            example[k] = example[k].strip()\n",
    "    return example\n",
    "\n",
    "\n",
    "def preprocess_for_t5(tokenizer: AutoTokenizer, examples: Dict[str, List[str]],\n",
    "                      max_input: int = 512, max_target: int = 128) -> Dict[str, Any]:\n",
    "    inputs = [\"summarize: \" + art for art in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"highlights\"],\n",
    "            max_length=max_target,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )[\"input_ids\"]\n",
    "    labels = [[-100 if tok == tokenizer.pad_token_id else tok for tok in seq] for seq in labels]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# 4) Unified PEFT model factory (T5): LoRA / AdaLoRA / QLoRA\n",
    "\n",
    "def create_t5_with_peft(\n",
    "    model_name: str = \"t5-base\",\n",
    "    target_modules: Optional[List[str]] = None,\n",
    "    mode: str = \"lora\",  # \"lora\", \"adalora\", \"qlora\"\n",
    "    lora_r: int = 16,\n",
    "    lora_alpha: int = 32,\n",
    "    lora_dropout: float = 0.05,\n",
    "    # AdaLoRA\n",
    "    adalora_init_r: int = 4,\n",
    "    adalora_tinit: int = 100,\n",
    "    adalora_tfinal: int = 1000,\n",
    "    total_steps: Optional[int] = None,\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token or \"<pad>\"\n",
    "\n",
    "    tmods = target_modules or [\"q\", \"k\", \"v\", \"o\", \"wi_0\", \"wi_1\", \"wo\"]\n",
    "\n",
    "    if mode.lower() == \"qlora\":\n",
    "        compute_dtype = (\n",
    "            torch.bfloat16\n",
    "            if (torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8)\n",
    "            else torch.float16\n",
    "        )\n",
    "        bnb_cfg = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=compute_dtype,\n",
    "        )\n",
    "        model = T5ForConditionalGeneration.from_pretrained(\n",
    "            model_name, quantization_config=bnb_cfg, device_map=\"auto\",\n",
    "        )\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        peft_cfg = LoraConfig(\n",
    "            r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
    "            bias=\"none\", task_type=\"SEQ_2_SEQ_LM\", target_modules=tmods,\n",
    "        )\n",
    "        quant_desc = \"4-bit (QLoRA)\"\n",
    "    else:\n",
    "        torch_dtype = (\n",
    "            torch.bfloat16\n",
    "            if (torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8)\n",
    "            else torch.float16\n",
    "        )\n",
    "        model = T5ForConditionalGeneration.from_pretrained(\n",
    "            model_name, torch_dtype=torch_dtype, device_map=\"auto\",\n",
    "        )\n",
    "        if mode.lower() == \"adalora\":\n",
    "            steps = int(total_steps or 1)\n",
    "\n",
    "            # derive a safe schedule from total_steps\n",
    "            tinit  = max(1, int(0.05 * steps))             # ~5% warmup\n",
    "            tfinal = max(tinit + 10, int(0.30 * steps))    # ~30% end of budgeting\n",
    "            if tfinal >= steps:\n",
    "                tfinal = max(tinit + 1, steps - 1)\n",
    "\n",
    "            # (optional) fallback for ultra-tiny runs\n",
    "            if steps < 20:\n",
    "                print(f\"[AdaLoRA] total_step={steps} too small; switching to plain LoRA.\")\n",
    "                peft_cfg = LoraConfig(\n",
    "                    r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
    "                    bias=\"none\", task_type=\"SEQ_2_SEQ_LM\", target_modules=tmods\n",
    "                )\n",
    "            else:\n",
    "                peft_cfg = AdaLoraConfig(\n",
    "                    r=lora_r,\n",
    "                    lora_alpha=lora_alpha,\n",
    "                    target_modules=tmods,\n",
    "                    init_r=adalora_init_r,\n",
    "                    tinit=tinit,\n",
    "                    tfinal=tfinal,\n",
    "                    total_step=steps,\n",
    "                    lora_dropout=lora_dropout,\n",
    "                    bias=\"none\",\n",
    "                    task_type=\"SEQ_2_SEQ_LM\",\n",
    "                )\n",
    "            print(f\"[AdaLoRA] steps={steps}, tinit={tinit}, tfinal={tfinal}\")\n",
    "            quant_desc = \"fp/bf16 (LoRA)\"\n",
    "        else:\n",
    "            peft_cfg = LoraConfig(\n",
    "                r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
    "                bias=\"none\", task_type=\"SEQ_2_SEQ_LM\", target_modules=tmods,\n",
    "            )\n",
    "            quant_desc = \"fp/bf16 (LoRA)\"\n",
    "\n",
    "    # training-friendly flags\n",
    "    model.config.use_cache = False\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "\n",
    "    model = get_peft_model(model, peft_cfg)\n",
    "\n",
    "    # safety guard\n",
    "    trainables = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    if trainables == 0:\n",
    "        raise RuntimeError(f\"Adapters not attached — check target_modules={tmods}\")\n",
    "\n",
    "    print(f\"Mode: {mode} | Base: {quant_desc}\")\n",
    "    model.print_trainable_parameters()\n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# 5) Training arguments & Trainer builder\n",
    "\n",
    "def make_training_args(\n",
    "    out_dir: str = \"./out/t5_peft\",\n",
    "    train_bs: int = 4,\n",
    "    eval_bs: int = 4,\n",
    "    epochs: int = 2,\n",
    "    lr: float = 2e-4,\n",
    "    logging_steps: int = 50,\n",
    ") -> TrainingArguments:\n",
    "    return TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        per_device_train_batch_size=train_bs,\n",
    "        per_device_eval_batch_size=eval_bs,\n",
    "        num_train_epochs=epochs,\n",
    "        learning_rate=lr,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=logging_steps,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        fp16=(torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] < 8),\n",
    "        bf16=(torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8),\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,\n",
    "        report_to=[\"none\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def build_trainer(model, tokenizer, train_ds, val_ds, args: TrainingArguments) -> Trainer:\n",
    "    collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100, padding=True)\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# 6) Metrics (ROUGE + BLEU)\n",
    "\n",
    "_rouge = evaluate.load(\"rouge\")\n",
    "_bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "\n",
    "def compute_text_metrics(preds: List[str], refs: List[str]) -> Dict[str, Any]:\n",
    "    r = _rouge.compute(predictions=preds, references=refs)\n",
    "    b = _bleu.compute(predictions=preds, references=refs)\n",
    "    return {\n",
    "        \"rouge1\": r.get(\"rouge1\"),\n",
    "        \"rouge2\": r.get(\"rouge2\"),\n",
    "        \"rougeL\": r.get(\"rougeL\"),\n",
    "        \"bleu\": b.get(\"bleu\"),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# 7) End-to-end: run PEFT on T5 (mode selectable)\n",
    "\n",
    "def run_t5_peft(\n",
    "    model_name: str = \"t5-base\",\n",
    "    sample_train: int = 2000,\n",
    "    out_dir: str = \"./out/t5_peft\",\n",
    "    epochs: int = 2,\n",
    "    train_bs: int = 4,\n",
    "    eval_bs: int = 4,\n",
    "    mode: str = \"lora\",  # \"lora\", \"adalora\", \"qlora\"\n",
    ") -> Dict[str, Any]:\n",
    "    # data\n",
    "    train_raw, val_raw = load_cnn_dm(sample_train=sample_train)\n",
    "    train_raw = train_raw.filter(lambda x: (x.get(\"highlights\") or \"\").strip() != \"\")\n",
    "    val_raw = val_raw.filter(lambda x: (x.get(\"highlights\") or \"\").strip() != \"\")\n",
    "    train_raw = train_raw.map(_strip_fields)\n",
    "    val_raw = val_raw.map(_strip_fields)\n",
    "\n",
    "    # model (estimate total steps for AdaLoRA scheduling)\n",
    "    est_steps = max(1, (len(train_raw) // max(1, train_bs)) * max(1, epochs))\n",
    "    tokenizer, model = create_t5_with_peft(\n",
    "        model_name=model_name,\n",
    "        mode=mode,\n",
    "        total_steps=est_steps,\n",
    "    )\n",
    "\n",
    "    # tokenize\n",
    "    train_tok = train_raw.map(lambda ex: preprocess_for_t5(tokenizer, ex), batched=True, remove_columns=train_raw.column_names)\n",
    "    val_tok = val_raw.map(lambda ex: preprocess_for_t5(tokenizer, ex), batched=True, remove_columns=val_raw.column_names)\n",
    "\n",
    "    # trainer\n",
    "    args = make_training_args(out_dir=out_dir, train_bs=train_bs, eval_bs=eval_bs, epochs=epochs)\n",
    "    trainer = build_trainer(model, tokenizer, train_tok, val_tok, args)\n",
    "\n",
    "    # quick sanity\n",
    "    sample = train_tok.select(range(min(4, len(train_tok))))\n",
    "    batch = trainer.data_collator(sample)\n",
    "    print(\"Batch keys:\", list(batch.keys()))\n",
    "\n",
    "    # train\n",
    "    t0 = time.time()\n",
    "    trainer.train()\n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    # generate on a few examples\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    few = val_raw.select(range(min(5, len(val_raw))))\n",
    "    prompts = [\"summarize: \" + s[\"article\"] for s in few]\n",
    "    enc = tokenizer(prompts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(DEVICE)\n",
    "\n",
    "    gen_beam = model.generate(**enc, max_new_tokens=128, num_beams=4)\n",
    "    gen_greedy = model.generate(**enc, max_new_tokens=128)\n",
    "    gen_sample = model.generate(**enc, max_new_tokens=128, do_sample=True, top_k=50, top_p=0.95)\n",
    "\n",
    "    pred_beam = tokenizer.batch_decode(gen_beam, skip_special_tokens=True)\n",
    "    pred_greedy = tokenizer.batch_decode(gen_greedy, skip_special_tokens=True)\n",
    "    pred_sample = tokenizer.batch_decode(gen_sample, skip_special_tokens=True)\n",
    "\n",
    "    refs = [s[\"highlights\"] for s in few]\n",
    "\n",
    "    m_beam = compute_text_metrics(pred_beam, refs)\n",
    "    m_greedy = compute_text_metrics(pred_greedy, refs)\n",
    "    m_sample = compute_text_metrics(pred_sample, refs)\n",
    "\n",
    "    return {\n",
    "        \"trainable_params\": count_trainable(model),\n",
    "        \"gpu_mem\": cuda_mem_stats(),\n",
    "        \"train_time_s\": train_time,\n",
    "        \"metrics\": {\n",
    "            \"beam\": m_beam,\n",
    "            \"greedy\": m_greedy,\n",
    "            \"sample\": m_sample,\n",
    "        },\n",
    "        \"examples\": {\n",
    "            \"inputs\": prompts,\n",
    "            \"beam\": pred_beam,\n",
    "            \"greedy\": pred_greedy,\n",
    "            \"sample\": pred_sample,\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "/tmp/ipykernel_16701/3814343219.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: lora | Base: fp/bf16 (LoRA)\n",
      "trainable params: 5,013,504 || all params: 227,917,056 || trainable%: 2.1997\n",
      "Batch keys: ['input_ids', 'attention_mask', 'labels', 'decoder_input_ids']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 05:09, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.635500</td>\n",
       "      <td>1.779909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.553000</td>\n",
       "      <td>1.777216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done.\n",
      "\n",
      "Trainable parameters: {'total': 227917056, 'trainable': 5013504, 'trainable_pct': 2.199705492861403}\n",
      "GPU memory: {'allocated_bytes': 524061184, 'max_allocated_bytes': 1012930560, 'reserved_bytes': 1130364928, 'max_reserved_bytes': 1130364928}\n",
      "Training time (s): 310.44\n",
      "\n",
      "Metrics (beam): {'rouge1': 0.2595959595959596, 'rouge2': 0.0673992673992674, 'rougeL': 0.19272727272727272, 'bleu': 0.05758046547802751}\n",
      "Metrics (greedy): {'rouge1': 0.28637765896288864, 'rouge2': 0.08303310051323051, 'rougeL': 0.2104155518674534, 'bleu': 0.07195675442095749}\n",
      "Metrics (sample): {'rouge1': 0.2482261966400138, 'rouge2': 0.0731620151434393, 'rougeL': 0.1728313358695596, 'bleu': 0.0}\n",
      "\n",
      "Examples:\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT: summarize: (CNN)Share, and your gift will be multiplied. That may sound like an esoteric adage, but when Zully Broussard selflessly decided to give one of her kidneys to a stranger, her generosity pai...\n",
      "BEAM: Zully Broussard gave one of her kidneys to a stranger . Her generosity paired up with big data . Six patients received transplants .\n",
      "GREEDY: Zully Broussard gave one kidney to a stranger, and her generosity was multiplied . Six patients received transplants, and the donor's kidney is not a match .\n",
      "SAMPLE: Uma Broussard, 40, gave kidney to stranger as part of big data project . Zully Broussard's gift led to six transplant recipients . Data processing allowed her to make massive donations .\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT: summarize: (CNN)On the 6th of April 1996, San Jose Clash and DC United strode out in front of 31,683 expectant fans at the Spartan Stadium in San Jose, California. The historic occasion was the first ...\n",
      "BEAM: Major League Soccer begins its 20th season in 2015 . Attendances are higher than ever before and the number of teams involved has doubled from 10 in the 1996 campaign to 20 in 2015 . Orlando City Soccer Club is a prime example of rapid growth in the first two decades of the MLS .\n",
      "GREEDY: Major League Soccer is set to begin its 20th season . Attendances are higher than ever before . The new season is the first of a new domestic TV and media rights deal worth $700 million .\n",
      "SAMPLE: San Jose Clash vs. DC United one of few historic MLS matches to get underway . Officials say attendances have skyrocketed in the first half of the 2015 season . MLS owner Phil Rawlins predicts \"the industry and the game itself has moved on dramatically\"\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT: summarize: (CNN)French striker Bafetimbi Gomis, who has a history of fainting, said he is now \"feeling well\" after collapsing during Swansea's 3-2 loss at Tottenham in the Premier League on Wednesday....\n",
      "BEAM: Bafetimbi Gomis says he is \"feeling well\" after collapsing during Swansea's 3-2 loss to Tottenham . Gomis spent the night in hospital as a precaution . Swansea manager Garry Monk says Gomis has a history of fainting .\n",
      "GREEDY: Bafetimbi Gomis says he is \"feeling well\" after collapsing during Swansea's 3-2 loss to Tottenham . The French striker has a history of fainting, and spent the night in hospital . Swansea manager Garry Monk says Gomis has \"no problems whatsoever\"\n",
      "SAMPLE: Bafetimbi Gomis says he is now feeling well after collapsing at Tottenham . Swansea striker has a history of fainting but is now in hospital .\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT: summarize: (CNN)It was an act of frustration perhaps more commonly associated with golf's fictional anti-hero Happy Gilmore than the world's reigning No 1. player. But when Rory McIlroy pulled his sec...\n",
      "BEAM: Rory McIlroy pulls his second shot on the eighth hole of the WGC Cadillac Championship into a lake . The four-time major winner jokes that the club \"must have gone a good 60, 70 yards\" McIlroy is one-under for the tournament and eight shots off the pace set by leader JB Holmes .\n",
      "GREEDY: Rory McIlroy pulls his shot into a lake at the Cadillac Championship . The four-time major winner is eight shots off the pace set by leader JB Holmes . McIlroy says he was frustrated by his shot on the eighth hole .\n",
      "SAMPLE: Rory McIlroy hits a second hole 70 for the WGC Cadillac Championship on Friday . PGA Tour chief says he has the frustration and desire to improve on last week . The northern Irishman bogeys his second shot into a lake on the eighth hole . Ryan Holmes is in second place on the course, two shots behind the world No 1.\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT: summarize: (CNN)A Pennsylvania community is pulling together to search for an eighth-grade student who has been missing since Wednesday. The search has drawn hundreds of volunteers on foot and online....\n",
      "BEAM: Cayman Naib, 13, was last seen wearing a gray down winter jacket, black ski pants and hiking boots . \"We think that he got a email from school and was upset by it and left as an impulsive act,\" his father says . Hundreds of volunteers have stepped up to pass out fliers and to canvass areas .\n",
      "GREEDY: Cayman Naib, 13, was last seen wearing a gray down winter jacket, black ski pants and hiking boots . His parents have been communicating through the Facebook group \"Find Cayman\"\n",
      "SAMPLE: NEW: The search will be taken to the skies Sunday with deployment of the Civil Air Patrol . Newtown Police say Cayman Naib was last seen wearing a gray down jacket, black ski pants . He's not wearing waterproof clothing, nor took his backpack, and is missing since Wednesday .\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    \"model_name\": \"t5-base\",      \n",
    "    \"sample_train\": 2000,         \n",
    "    \"out_dir\": \"./out/t5_lora\",\n",
    "    \"epochs\": 2,\n",
    "    \"train_bs\": 4,\n",
    "    \"eval_bs\": 4,\n",
    "    \"mode\": \"lora\",            \n",
    "}\n",
    "\n",
    "results = run_t5_peft(\n",
    "    model_name=CONFIG[\"model_name\"],\n",
    "    sample_train=CONFIG[\"sample_train\"],\n",
    "    out_dir=CONFIG[\"out_dir\"],\n",
    "    epochs=CONFIG[\"epochs\"],\n",
    "    train_bs=CONFIG[\"train_bs\"],\n",
    "    eval_bs=CONFIG[\"eval_bs\"],\n",
    "    mode=CONFIG[\"mode\"],\n",
    ")\n",
    "print(\"Training done.\\n\")\n",
    "\n",
    "print(\"Trainable parameters:\", results[\"trainable_params\"])\n",
    "print(\"GPU memory:\", results[\"gpu_mem\"]) \n",
    "print(\"Training time (s):\", round(results[\"train_time_s\"], 2))\n",
    "print(\"\\nMetrics (beam):\", results[\"metrics\"][\"beam\"]) \n",
    "print(\"Metrics (greedy):\", results[\"metrics\"][\"greedy\"]) \n",
    "print(\"Metrics (sample):\", results[\"metrics\"][\"sample\"]) \n",
    "\n",
    "print(\"\\nExamples:\")\n",
    "for i, src in enumerate(results[\"examples\"][\"inputs\"]):\n",
    "    print(\"-\" * 80)\n",
    "    print(\"INPUT:\", src[:200].replace(\"\\n\", \" \") + (\"...\" if len(src) > 200 else \"\"))\n",
    "    print(\"BEAM:\", results[\"examples\"][\"beam\"][i])\n",
    "    print(\"GREEDY:\", results[\"examples\"][\"greedy\"][i])\n",
    "    print(\"SAMPLE:\", results[\"examples\"][\"sample\"][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages/peft/tuners/adalora/config.py:96: UserWarning: Note that `r` is not used in AdaLora and will be ignored.If you intended to set the initial rank, use `init_r` instead.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_16701/3814343219.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AdaLoRA] steps=1000, tinit=50, tfinal=300\n",
      "Mode: adalora | Base: fp/bf16 (LoRA)\n",
      "trainable params: 1,254,048 || all params: 224,157,768 || trainable%: 0.5594\n",
      "Batch keys: ['input_ids', 'attention_mask', 'labels', 'decoder_input_ids']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 07:24, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.701400</td>\n",
       "      <td>1.819320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.627600</td>\n",
       "      <td>1.802031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done.\n",
      "\n",
      "Trainable parameters: {'total': 224157768, 'trainable': 1254048, 'trainable_pct': 0.5594488253469762}\n",
      "GPU memory: {'allocated_bytes': 485116416, 'max_allocated_bytes': 1075312640, 'reserved_bytes': 1088421888, 'max_reserved_bytes': 1231028224}\n",
      "Training time (s): 444.88\n",
      "\n",
      "Metrics (beam): {'rouge1': 0.2327980206927575, 'rouge2': 0.0643394655159361, 'rougeL': 0.1726707795128848, 'bleu': 0.05382795411296747}\n",
      "Metrics (greedy): {'rouge1': 0.3382428051976542, 'rouge2': 0.12107612249654101, 'rougeL': 0.2699932019763703, 'bleu': 0.091354210737194}\n",
      "Metrics (sample): {'rouge1': 0.20232671230314594, 'rouge2': 0.05703809851350835, 'rougeL': 0.15766975594155486, 'bleu': 0.03519468025439135}\n",
      "\n",
      "Examples:\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT: summarize: (CNN)Share, and your gift will be multiplied. That may sound like an esoteric adage, but when Zully Broussard selflessly decided to give one of her kidneys to a stranger, her generosity pai...\n",
      "BEAM: Zully Broussard gave one of her kidneys to a stranger . Her generosity was multiplied by data processing from donor-recipient pairs . So high, it is taking five surgeons, a covey of physician assistants, nurses and anesthesiologists to perform surgeries on 12 people .\n",
      "GREEDY: Zully Broussard gave one kidney to a stranger, and her generosity was multiplied . Six patients received transplants, and the donor's kidneys were extracted .\n",
      "SAMPLE: Uma Broussard, 40, gave kidney to stranger as reward for generosity . she got her gifts from 12 donors, six recipients . \"I know this entire journey is much bigger than all of us,\" she said .\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT: summarize: (CNN)On the 6th of April 1996, San Jose Clash and DC United strode out in front of 31,683 expectant fans at the Spartan Stadium in San Jose, California. The historic occasion was the first ...\n",
      "BEAM: As the MLS prepares to mark the beginning of its 20th season, it's hard to comprehend just how much the league has progressed . Attendances are higher than ever before and the number of teams involved has doubled from 10 in the 1996 campaign to 20 in 2015 .\n",
      "GREEDY: Major League Soccer is entering its 20th season . attendances have increased from 10 in 1996 to 20 in 2015 . Orlando City Soccer Club is one of the fastest growing MLS franchises .\n",
      "SAMPLE: San Jose Clash vs. DC United is the first major league soccer match since 1996 . attendances are higher than ever before at the beginning of the new MLS season .\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT: summarize: (CNN)French striker Bafetimbi Gomis, who has a history of fainting, said he is now \"feeling well\" after collapsing during Swansea's 3-2 loss at Tottenham in the Premier League on Wednesday....\n",
      "BEAM: Bafetimbi Gomis says he is now \"feeling well\" after collapsing during Swansea's 3-2 loss to Tottenham . Gomis has a history of fainting and spent the night in hospital as a precaution .\n",
      "GREEDY: Bafetimbi Gomis says he is \"feeling well\" after collapsing during Swansea's 3-2 loss to Tottenham . Gomis has a history of fainting, and spent the night in hospital as a precaution . Swansea manager Garry Monk says Gomis has \"no problems whatsoever\"\n",
      "SAMPLE: Bafetimbi Gomis says he is \"feeling well\" after collapsing during Swansea's loss at Tottenham . Gomis had similar fainting spells in France, which prompted French president to tell franco-France TV .\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT: summarize: (CNN)It was an act of frustration perhaps more commonly associated with golf's fictional anti-hero Happy Gilmore than the world's reigning No 1. player. But when Rory McIlroy pulled his sec...\n",
      "BEAM: Rory McIlroy pulls his second shot on the eighth hole of the WGC Cadillac Championship into a lake . \"I just let frustration get the better of me,\" the four-time major winner jokes . McIlroy finishes one-under for the tournament and eight shots off the pace set by leader JB Holmes .\n",
      "GREEDY: Rory McIlroy pulls his shot into a lake at the Cadillac Championship . the northern Irishman is eight shots off the pace of leader JB Holmes .\n",
      "SAMPLE: Rory McIlroy hits a second hole 70 for the WGC Cadillac Championship on Friday . he remains eight shots off the pace of current leader JB Holmes at one-under-par . PGA Tour member says the incident echoes the same mythological scene of Happy Gilmore .\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT: summarize: (CNN)A Pennsylvania community is pulling together to search for an eighth-grade student who has been missing since Wednesday. The search has drawn hundreds of volunteers on foot and online....\n",
      "BEAM: Cayman Naib, 13, was last seen wearing a gray down winter jacket, black ski pants and hiking boots . \"We think that he got a email from school and was upset by it and left as an impulsive act,\" his father says . Hundreds of volunteers have stepped up to pass out fliers and to canvass areas .\n",
      "GREEDY: Cayman Naib, 13, has been missing since Wednesday . he was last seen wearing a gray down winter jacket, black ski pants and hiking boots .\n",
      "SAMPLE: 13-year-old Cayman Naib was last seen wearing a gray down winter jacket, black ski pants, hiking boots . Hundreds of volunteers have stepped up on foot and online to send out fliers . Hundreds of volunteers have stepped up to pass out fliers and to canvass areas, according to posts .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CONFIG = {\n",
    "    \"model_name\": \"t5-base\",     \n",
    "    \"sample_train\": 2000,        \n",
    "    \"out_dir\": \"./out/t5_adalora\",\n",
    "    \"epochs\": 2,\n",
    "    \"train_bs\": 4,\n",
    "    \"eval_bs\": 4,\n",
    "    \"mode\": \"adalora\",         \n",
    "}\n",
    "\n",
    "results = run_t5_peft(\n",
    "    model_name=CONFIG[\"model_name\"],\n",
    "    sample_train=CONFIG[\"sample_train\"],\n",
    "    out_dir=CONFIG[\"out_dir\"],\n",
    "    epochs=CONFIG[\"epochs\"],\n",
    "    train_bs=CONFIG[\"train_bs\"],\n",
    "    eval_bs=CONFIG[\"eval_bs\"],\n",
    "    mode=CONFIG[\"mode\"],\n",
    ")\n",
    "print(\"Training done.\\n\")\n",
    "\n",
    "print(\"Trainable parameters:\", results[\"trainable_params\"])\n",
    "print(\"GPU memory:\", results[\"gpu_mem\"]) \n",
    "print(\"Training time (s):\", round(results[\"train_time_s\"], 2))\n",
    "print(\"\\nMetrics (beam):\", results[\"metrics\"][\"beam\"]) \n",
    "print(\"Metrics (greedy):\", results[\"metrics\"][\"greedy\"]) \n",
    "print(\"Metrics (sample):\", results[\"metrics\"][\"sample\"]) \n",
    "\n",
    "print(\"\\nExamples:\")\n",
    "for i, src in enumerate(results[\"examples\"][\"inputs\"]):\n",
    "    print(\"-\" * 80)\n",
    "    print(\"INPUT:\", src[:200].replace(\"\\n\", \" \") + (\"...\" if len(src) > 200 else \"\"))\n",
    "    print(\"BEAM:\", results[\"examples\"][\"beam\"][i])\n",
    "    print(\"GREEDY:\", results[\"examples\"][\"greedy\"][i])\n",
    "    print(\"SAMPLE:\", results[\"examples\"][\"sample\"][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: qlora | Base: 4-bit (QLoRA)\n",
      "trainable params: 5,013,504 || all params: 227,917,056 || trainable%: 2.1997\n",
      "Batch keys: ['input_ids', 'attention_mask', 'labels', 'decoder_input_ids']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16701/3814343219.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 07:07, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.661400</td>\n",
       "      <td>1.791404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.572600</td>\n",
       "      <td>1.787696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done.\n",
      "\n",
      "Trainable parameters: {'total': 157138176, 'trainable': 5013504, 'trainable_pct': 3.1905066786571328}\n",
      "GPU memory: {'allocated_bytes': 479298560, 'max_allocated_bytes': 1426023424, 'reserved_bytes': 1545601024, 'max_reserved_bytes': 1545601024}\n",
      "Training time (s): 427.91\n",
      "\n",
      "Metrics (beam): {'rouge1': 0.25060399917542775, 'rouge2': 0.07116201957369696, 'rougeL': 0.18386312100597815, 'bleu': 0.06103131119773303}\n",
      "Metrics (greedy): {'rouge1': 0.3072875759646112, 'rouge2': 0.10165843974740738, 'rougeL': 0.24671616893623644, 'bleu': 0.07539425731206427}\n",
      "Metrics (sample): {'rouge1': 0.2827272727272727, 'rouge2': 0.07000107851596203, 'rougeL': 0.20396976408604311, 'bleu': 0.0375466694474461}\n",
      "\n",
      "Examples:\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT: summarize: (CNN)Share, and your gift will be multiplied. That may sound like an esoteric adage, but when Zully Broussard selflessly decided to give one of her kidneys to a stranger, her generosity pai...\n",
      "BEAM: Zully Broussard gave one of her kidneys to a stranger . Her generosity paired up with big data, and six patients received transplants .\n",
      "GREEDY: Zully Broussard gave one kidney to a stranger, and her generosity paired up with big data . Six patients received transplants, and the process is taking five surgeons, 40 staff .\n",
      "SAMPLE: Uma Broussard, 40, gave kidney to a stranger . Her generosity multiplied and gave 12 patients six transplants .\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT: summarize: (CNN)On the 6th of April 1996, San Jose Clash and DC United strode out in front of 31,683 expectant fans at the Spartan Stadium in San Jose, California. The historic occasion was the first ...\n",
      "BEAM: Major League Soccer begins its 20th season in 2015 . Attendances are higher than ever before and the number of teams involved has doubled . The new season is the first of a new domestic TV and media rights deal with FOX, ESPN and Univision worth $700 million over eight years .\n",
      "GREEDY: The MLS is set to begin its 20th season this year . The league has seen a rapid transformation in the past two decades . The new season is the first of a new domestic TV and media rights deal worth $700 million .\n",
      "SAMPLE: America's favorite soccer league will start its 20th season this week . MLS attendance has tripled from 10 teams in 1996 to 20 teams in 2015 . MLS owner believes industry has moved on a lot during the first 20 years . \"The industry has moved on dramatically,\" says Orlando City Soccer Club owner .\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT: summarize: (CNN)French striker Bafetimbi Gomis, who has a history of fainting, said he is now \"feeling well\" after collapsing during Swansea's 3-2 loss at Tottenham in the Premier League on Wednesday....\n",
      "BEAM: Bafetimbi Gomis says he is now \"feeling well\" after collapsing during Swansea's 3-2 loss to Tottenham . Gomis has a history of fainting and spent the night in hospital as a precaution . Swansea manager Garry Monk says Gomis' condition is \"not as serious as it looks\"\n",
      "GREEDY: Bafetimbi Gomis says he is now \"feeling well\" after collapsing during Swansea's 3-2 loss to Tottenham . Gomis has a history of fainting, with the club's president telling French tv in 2009 . Swansea manager Garry Monk says Gomis has had \"no problems whatsoever\"\n",
      "SAMPLE: Welsh striker Bafetimbi Gomis said he is now feeling well following his collapse . The 29-year-old collapsed during Swansea's defeat to Tottenham in the Premier League . Gomis says he has a history of fainting that has caused him to need hospitalization .\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT: summarize: (CNN)It was an act of frustration perhaps more commonly associated with golf's fictional anti-hero Happy Gilmore than the world's reigning No 1. player. But when Rory McIlroy pulled his sec...\n",
      "BEAM: Rory McIlroy pulls his second shot on the eighth hole of the WGC Cadillac Championship into a lake . The four-time major winner jokes that the club \"must have gone a good 60, 70 yards\" McIlroy is one-under for the tournament and eight shots off the pace set by leader JB Holmes .\n",
      "GREEDY: Rory McIlroy pulls his second shot into a lake at the Cadillac Championship . The four-time major winner jokes that frustration got the better of him . McIlroy is eight shots off the pace set by world No.1 JB Holmes .\n",
      "SAMPLE: Rory McIlroy hits a second hole of the Cadillac Championship with a dropped ball . PGA Tour chief says he has the frustration and desire to improve on last week . The world No 1 bogeys his second shot into a lake on the eighth hole of the tournament .\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT: summarize: (CNN)A Pennsylvania community is pulling together to search for an eighth-grade student who has been missing since Wednesday. The search has drawn hundreds of volunteers on foot and online....\n",
      "BEAM: Cayman Naib, 13, was last seen wearing a gray down winter jacket, black ski pants and hiking boots . Hundreds of volunteers have stepped up to pass out fliers and to canvass areas .\n",
      "GREEDY: Cayman Naib, 13, has been missing since Wednesday . He could be in the Radnor-Wayne area, roughly 20 miles from Philadelphia .\n",
      "SAMPLE: NEW: The search will be taken to the skies Sunday, a civil air patrol official said . Cayman Naib is believed to have gone missing on Wednesday in northeast Pennsylvania . Police say he was last seen wearing a gray down jacket, black ski pants and hiking boots .\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    \"model_name\": \"t5-base\",    \n",
    "    \"sample_train\": 2000,       \n",
    "    \"out_dir\": \"./out/t5_qlora\",\n",
    "    \"epochs\": 2,\n",
    "    \"train_bs\": 4,\n",
    "    \"eval_bs\": 4,\n",
    "    \"mode\": \"qlora\",          \n",
    "}\n",
    "\n",
    "results = run_t5_peft(\n",
    "    model_name=CONFIG[\"model_name\"],\n",
    "    sample_train=CONFIG[\"sample_train\"],\n",
    "    out_dir=CONFIG[\"out_dir\"],\n",
    "    epochs=CONFIG[\"epochs\"],\n",
    "    train_bs=CONFIG[\"train_bs\"],\n",
    "    eval_bs=CONFIG[\"eval_bs\"],\n",
    "    mode=CONFIG[\"mode\"],\n",
    ")\n",
    "print(\"Training done.\\n\")\n",
    "\n",
    "print(\"Trainable parameters:\", results[\"trainable_params\"])\n",
    "print(\"GPU memory:\", results[\"gpu_mem\"]) \n",
    "print(\"Training time (s):\", round(results[\"train_time_s\"], 2))\n",
    "print(\"\\nMetrics (beam):\", results[\"metrics\"][\"beam\"]) \n",
    "print(\"Metrics (greedy):\", results[\"metrics\"][\"greedy\"]) \n",
    "print(\"Metrics (sample):\", results[\"metrics\"][\"sample\"]) \n",
    "\n",
    "print(\"\\nExamples:\")\n",
    "for i, src in enumerate(results[\"examples\"][\"inputs\"]):\n",
    "    print(\"-\" * 80)\n",
    "    print(\"INPUT:\", src[:200].replace(\"\\n\", \" \") + (\"...\" if len(src) > 200 else \"\"))\n",
    "    print(\"BEAM:\", results[\"examples\"][\"beam\"][i])\n",
    "    print(\"GREEDY:\", results[\"examples\"][\"greedy\"][i])\n",
    "    print(\"SAMPLE:\", results[\"examples\"][\"sample\"][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2: Применение различных PEFT методов для задачи машинного перевода\n",
    "\n",
    "**Цель задания**: Сравнить эффективность различных Parameter Efficient Fine-Tuning (PEFT) подходов для задачи автоматического перевода с английского на русский. В рамках задания необходимо реализовать и сравнить минимум 3 различных PEFT метода, проанализировать их влияние на качество перевода и эффективность обучения.\n",
    "\n",
    "#### Задачи:\n",
    "\n",
    "1. **Выбор датасета**:\n",
    "   - Загрузите параллельный датасет для перевода, например, [Opus Books](https://huggingface.co/datasets/Helsinki-NLP/opus_books/viewer/en-ru), который содержит тексты на английском языке и их переводы на русский.\n",
    "   - Используйте библиотеку `datasets` для загрузки и обработки данных. Убедитесь, что данные содержат параллельные тексты для обучения модели переводу.\n",
    "\n",
    "2. **Предобработка данных**:\n",
    "   - **Разделите данные** на обучающую и тестовую выборки (например, 80% для обучения и 20% для тестирования).\n",
    "   - **Очистите текст**, удалив лишние пробелы и специальные символы, которые могут повлиять на обучение модели.\n",
    "   - **Подготовьте данные** в формате, подходящем для выбранной модели:\n",
    "     - Для GPT-2 данные должны быть в виде последовательности токенов, где исходный текст и перевод разделены специальными символами (например, `<|startoftext|>` для начала текста и `<|endoftext|>` для его конца).\n",
    "     - Для T5 данные подаются в формате задачи перевода: входной текст начинается с задания `\"translate English to Russian: <текст на английском>\"`, а на выходе модель должна сгенерировать перевод.\n",
    "\n",
    "3. **Создание модели**:\n",
    "   - **GPT-2**:\n",
    "     - Импортируйте предобученную модель `GPT2LMHeadModel` из библиотеки Hugging Face.\n",
    "     - GPT-2 изначально не обучена для задачи перевода, поэтому нужно будет использовать специальную подготовку данных и дообучение на параллельных текстах.\n",
    "     - Убедитесь, что модель настроена для генерации текста, ограничивая длину вывода для перевода.\n",
    "   \n",
    "   - **T5**:\n",
    "     - Импортируйте модель `T5ForConditionalGeneration`, которая предобучена на множестве задач, включая перевод. T5 — это модель с условной генерацией, которая использует специальную задачу (`task`) для перевода.\n",
    "     - Подготовьте модель для выполнения задачи перевода с английского на русский, используя предобученные веса и формат входных данных.\n",
    "\n",
    "4. **Настройка параметров обучения**:\n",
    "   - Настройте параметры обучения для обеих моделей:\n",
    "     - Количество эпох (например, 3-5 эпох).\n",
    "     - Размер батча (например, 16 или 32).\n",
    "     - Скорость обучения (рекомендуется начать с 5e-5 и адаптировать в зависимости от потерь на валидации).\n",
    "   - Используйте подходящие оптимизаторы, такие как AdamW, и функцию потерь для задачи перевода:\n",
    "     - Для T5 подойдёт стандартная кросс-энтропийная функция потерь.\n",
    "     - Для GPT-2 используйте ту же функцию с учётом последовательной генерации текста (автогрегрессии).\n",
    "\n",
    "5. **Сравнительное исследование PEFT методов** (ключевая часть задания):\n",
    "   - **Обязательно** реализуйте и сравните следующие PEFT подходы:\n",
    "     - **LoRA**: Настройте различные значения rank (4, 8, 16, 32), alpha (16, 32, 64)\n",
    "     - **QLoRA**: Комбинация 4-bit квантизации с LoRA для экономии памяти\n",
    "     - **AdaLoRA**: Адаптивное изменение ранга с бюджетом параметров\n",
    "     - **Дополнительные методы** (на выбор): IA³, Prefix Tuning, P-Tuning v2, или (IA)³\n",
    "   - Для каждого PEFT метода проведите **grid search** по ключевым гиперпараметрам\n",
    "   - **Baseline**: обязательно сравните с полным fine-tuning (если позволяют ресурсы)\n",
    "   - Зафиксируйте для каждого эксперимента:\n",
    "     - Процент обучаемых параметров от общего числа\n",
    "     - Потребление GPU памяти (в GB)\n",
    "     - Время обучения на эпоху\n",
    "     - Скорость инференса (токенов/сек)\n",
    "   - Используйте `SFTTrainer` или `Trainer` для стабильного процесса обучения\n",
    "\n",
    "6. **Инференс**:\n",
    "   - Используйте дообученную модель для перевода текстов с английского на русский на тестовой выборке.\n",
    "   - Подготовьте несколько примеров для перевода и выведите результаты:\n",
    "     - Для GPT-2 используйте автогрегрессивное декодирование текста.\n",
    "     - Для T5 применяйте стандартные стратегии декодирования (например, greedy decoding или beam search).\n",
    "   \n",
    "7. **Комплексная оценка и анализ PEFT методов**:\n",
    "   - **Автоматические метрики**:\n",
    "     - **BLEU** (corpus-level и sentence-level) — основная метрика для машинного перевода\n",
    "     - **chrF** — character-level F-score для более точной оценки морфологически богатых языков\n",
    "     - **COMET** — нейронная метрика качества перевода (если позволяют ресурсы)\n",
    "     - **ROUGE** — для дополнительной оценки похожести\n",
    "   - **Создайте детальную сравнительную таблицу**:\n",
    "     - Эффективность (% параметров, время, память) vs Качество (BLEU, chrF)\n",
    "     - Pareto-frontier анализ: какие методы обеспечивают лучший trade-off\n",
    "   - **Качественный анализ**: \n",
    "     - Проанализируйте примеры переводов от каждого PEFT метода\n",
    "     - Определите типы ошибок, характерные для каждого подхода\n",
    "     - Оцените стабильность качества на различных типах текстов\n",
    "\n",
    "#### Ожидаемые результаты:\n",
    "- **Исследовательский код** с реализацией всех протестированных PEFT методов\n",
    "- **Научно-обоснованный отчет** с детальным сравнением эффективности PEFT подходов\n",
    "- **Визуализации**: графики Pareto-frontier, сравнительные диаграммы по метрикам\n",
    "- **Практические рекомендации**: когда использовать каждый PEFT метод в зависимости от ограничений\n",
    "- **Воспроизводимые результаты**: четкие инструкции по повторению экспериментов\n",
    "\n",
    "#### Рекомендуемые ресурсы:\n",
    "- **[PEFT Documentation](https://huggingface.co/docs/peft/index)** - полная документация библиотеки PEFT\n",
    "- **[LoRA Paper & Implementation](https://arxiv.org/abs/2106.09685)** - оригинальная статья LoRA\n",
    "- **[QLoRA Paper](https://arxiv.org/abs/2305.14314)** - квантизованный LoRA подход\n",
    "- **[AdaLoRA Paper](https://arxiv.org/abs/2303.10512)** - адаптивный LoRA\n",
    "- **[PEFT Task Guides](https://huggingface.co/docs/peft/task_guides/translation)** - гайды по применению PEFT для разных задач\n",
    "- [Документация Hugging Face Transformers](https://huggingface.co/docs/transformers/index)\n",
    "- [Документация Hugging Face Datasets](https://huggingface.co/docs/datasets/index)\n",
    "- **[BitsAndBytes](https://huggingface.co/docs/bitsandbytes/index)** - для квантизации в QLoRA\n",
    "- **[TRL Library](https://huggingface.co/docs/trl/index)** - для эффективного обучения\n",
    "\n",
    "#### Критерии оценки:\n",
    "- **Корректность реализации PEFT методов** (35%) - правильная настройка минимум 3 различных PEFT подходов\n",
    "- **Качество сравнительного анализа** (25%) - детальное сравнение по всем метрикам эффективности и качества\n",
    "- **Научная обоснованность выводов** (20%) - аргументированные рекомендации по выбору методов\n",
    "- **Воспроизводимость результатов** (10%) - четкие инструкции и фиксированные семена\n",
    "- **Качество перевода** (10%) - достижение конкурентоспособных результатов по BLEU/chrF метрикам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "from __future__ import annotations\n",
    "import re, random\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import cuda\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, GPT2Tokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "\n",
    "# PEFT\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    AdaLoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "import evaluate\n",
    "import sacrebleu\n",
    "from sacrebleu.metrics import CHRF\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Data\n",
    "    dataset_name: str = \"Helsinki-NLP/opus_books\"\n",
    "    dataset_subset: str = \"en-ru\"\n",
    "    test_size: float = 0.2\n",
    "    seed: int = 42\n",
    "    # Model / Tokenizer\n",
    "    base_model: str = \"gpt2\"\n",
    "    max_length: int = 512\n",
    "    # LoRA / AdaLoRA hyperparams\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.1\n",
    "    adalora_target_r: int = 8\n",
    "    adalora_init_r: int = 12\n",
    "    adalora_tinit: int = 200\n",
    "    adalora_tfinal: int = 1000\n",
    "    adalora_deltaT: int = 10\n",
    "    # QLoRA (bnb) config\n",
    "    qlora_4bit: bool = True\n",
    "    qlora_compute_dtype: torch.dtype = torch.bfloat16\n",
    "    # Training\n",
    "    output_dir_base: str = \"./gpt2_peft_mt\"\n",
    "    epochs: int = 5\n",
    "    train_batch_size: int = 10\n",
    "    eval_batch_size: int = 10\n",
    "    grad_accum_steps: int = 8\n",
    "    lr: float = 5e-5\n",
    "    fp16: bool = False\n",
    "    logging_steps: int = 50\n",
    "    save_total_limit: int = 2\n",
    "    eval_steps: int = 500\n",
    "    # Generation for eval\n",
    "    gen_max_new_tokens: int = 128\n",
    "    gen_num_beams: int = 5\n",
    "    # Eval subset size\n",
    "    eval_samples: int = 500\n",
    "\n",
    "CFG = Config()\n",
    "random.seed(CFG.seed)\n",
    "DEVICE = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ### 2) Data loading & preparation helpers\n",
    "\n",
    "# %%\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def load_and_prepare_data(cfg: Config) -> Tuple[DatasetDict, Dataset, Dataset]:\n",
    "    raw = load_dataset(cfg.dataset_name, cfg.dataset_subset)\n",
    "    rows = [\n",
    "        {\"en\": clean_text(x[\"translation\"][\"en\"]), \"ru\": clean_text(x[\"translation\"][\"ru\"])}\n",
    "        for x in raw[\"train\"]\n",
    "        if x[\"translation\"][\"en\"] is not None and x[\"translation\"][\"ru\"] is not None\n",
    "    ]\n",
    "    train_rows, test_rows = train_test_split(rows, test_size=cfg.test_size, random_state=cfg.seed)\n",
    "    train_ds = Dataset.from_list(train_rows)\n",
    "    test_ds = Dataset.from_list(test_rows)\n",
    "    ds = DatasetDict({\"train\": train_ds, \"test\": test_ds})\n",
    "    return ds, train_ds, test_ds\n",
    "\n",
    "def build_prompt(example: Dict[str, str]) -> str:\n",
    "    # Causal-LM as Seq2Seq via tags\n",
    "    return f\"<|startoftext|> {example['en']} <|sep|> {example['ru']} <|endoftext|>\"\n",
    "\n",
    "def to_causal_format(ds: Dataset) -> Dataset:\n",
    "    return ds.map(lambda ex: {\"text\": build_prompt(ex)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ### 3) Tokenizer & Base Model (no PEFT applied yet)\n",
    "# This cell only creates the tokenizer and a **base** GPT-2 model.\n",
    "# PEFT adapters will be attached in the dedicated LoRA/AdaLoRA/QLoRA cells.\n",
    "\n",
    "# %%\n",
    "def load_tokenizer(cfg: Config) -> GPT2Tokenizer:\n",
    "    tok = GPT2Tokenizer.from_pretrained(cfg.base_model)\n",
    "    special_tokens = {\n",
    "        \"pad_token\": \"<|pad|>\",\n",
    "        \"bos_token\": \"<|startoftext|>\",\n",
    "        \"eos_token\": \"<|endoftext|>\",\n",
    "        \"additional_special_tokens\": [\"<|sep|>\"],\n",
    "    }\n",
    "    tok.add_special_tokens(special_tokens)\n",
    "    return tok\n",
    "\n",
    "def load_base_model(cfg: Config, tokenizer: GPT2Tokenizer, quantization_config=None) -> GPT2LMHeadModel:\n",
    "    model = GPT2LMHeadModel.from_pretrained(\n",
    "        cfg.base_model,\n",
    "        device_map=\"auto\" if quantization_config is not None else None,\n",
    "        quantization_config=quantization_config,\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ### 4) Tokenization, Generation, and Metrics\n",
    "# Common utilities used by all three methods.\n",
    "\n",
    "# %%\n",
    "def tokenize_fn(tokenizer: GPT2Tokenizer, max_length: int):\n",
    "    def _tok(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, max_length=max_length)\n",
    "    return _tok\n",
    "\n",
    "def generate_translation(model: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, src_en: str, cfg: Config) -> str:\n",
    "    model.eval()\n",
    "    prompt = f\"<|startoftext|> {src_en} <|sep|>\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=cfg.gen_max_new_tokens,\n",
    "            num_beams=cfg.gen_num_beams,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    text = tokenizer.decode(out_ids[0], skip_special_tokens=False)\n",
    "    if \"<|sep|>\" in text:\n",
    "        text = text.split(\"<|sep|>\", 1)[1]\n",
    "    return text.replace(\"<|endoftext|>\", \"\").replace(\"<|pad|>\", \"\").strip()\n",
    "\n",
    "def compute_metrics_on_subset(model, tokenizer, test_raw: Dataset, cfg: Config) -> Dict[str, float]:\n",
    "    n = min(len(test_raw), cfg.eval_samples)\n",
    "    sample = test_raw.shuffle(seed=CFG.seed).select(range(n))\n",
    "    preds, refs = [], []\n",
    "    for ex in sample:\n",
    "        src, ref = ex[\"en\"], ex[\"ru\"]\n",
    "        hyp = generate_translation(model, tokenizer, src, cfg)\n",
    "        preds.append(hyp)\n",
    "        refs.append(ref)\n",
    "\n",
    "    bleu = sacrebleu.corpus_bleu(preds, [refs]).score\n",
    "    chrf = CHRF().corpus_score(preds, [refs]).score\n",
    "    rouge_metric = evaluate.load(\"rouge\")\n",
    "    rouge_res = rouge_metric.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "    return {\n",
    "        \"BLEU\": bleu,\n",
    "        \"chrF\": chrf,\n",
    "        \"ROUGE-1\": rouge_res.get(\"rouge1\", 0.0) * 100,\n",
    "        \"ROUGE-2\": rouge_res.get(\"rouge2\", 0.0) * 100,\n",
    "        \"ROUGE-L\": rouge_res.get(\"rougeL\", 0.0) * 100,\n",
    "        \"ROUGE-Lsum\": rouge_res.get(\"rougeLsum\", 0.0) * 100,\n",
    "    }\n",
    "\n",
    "def build_trainer(model, tokenizer, train_tok, test_tok, cfg: Config, out_dir_suffix: str) -> Trainer:\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"{cfg.output_dir_base}/{out_dir_suffix}\",\n",
    "        per_device_train_batch_size=cfg.train_batch_size,\n",
    "        per_device_eval_batch_size=cfg.eval_batch_size,\n",
    "        gradient_accumulation_steps=cfg.grad_accum_steps,\n",
    "        num_train_epochs=cfg.epochs,\n",
    "        learning_rate=cfg.lr,\n",
    "        fp16=cfg.fp16,\n",
    "        logging_steps=cfg.logging_steps,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=cfg.eval_steps,\n",
    "        save_steps=cfg.eval_steps,\n",
    "        save_total_limit=cfg.save_total_limit,\n",
    "        report_to=\"none\",\n",
    "        load_best_model_at_end=False,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_tok,\n",
    "        eval_dataset=test_tok,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'dataset_name': 'Helsinki-NLP/opus_books', 'dataset_subset': 'en-ru', 'test_size': 0.2, 'seed': 42, 'base_model': 'gpt2', 'max_length': 512, 'lora_r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1, 'adalora_target_r': 8, 'adalora_init_r': 12, 'adalora_tinit': 200, 'adalora_tfinal': 1000, 'adalora_deltaT': 10, 'qlora_4bit': True, 'qlora_compute_dtype': torch.bfloat16, 'output_dir_base': './gpt2_peft_mt', 'epochs': 5, 'train_batch_size': 10, 'eval_batch_size': 10, 'grad_accum_steps': 8, 'lr': 5e-05, 'fp16': False, 'logging_steps': 50, 'save_total_limit': 2, 'eval_steps': 500, 'gen_max_new_tokens': 128, 'gen_num_beams': 5, 'eval_samples': 500}\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['en', 'ru'],\n",
      "        num_rows: 13996\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['en', 'ru'],\n",
      "        num_rows: 3500\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 13996/13996 [00:00<00:00, 38339.80 examples/s]\n",
      "Map: 100%|██████████| 3500/3500 [00:00<00:00, 37433.77 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample formatted: <|startoftext|> The rye – after he had so long held out for a certain price – was sold fifty kopeks a chetvert cheaper than had been offered him a month ago. <|sep|> Рожь, цену на которую он так долго\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 13996/13996 [00:06<00:00, 2051.20 examples/s]\n",
      "Map: 100%|██████████| 3500/3500 [00:01<00:00, 1884.40 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokenized columns: ['input_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ### 5) Prepare Dataset Once (used by all methods)\n",
    "# Run this once; it caches tokenized data for reuse.\n",
    "\n",
    "# %%\n",
    "print(\"Config:\", asdict(CFG))\n",
    "ds, train_raw, test_raw = load_and_prepare_data(CFG)\n",
    "print(ds)\n",
    "\n",
    "train_fmt = to_causal_format(train_raw)\n",
    "test_fmt  = to_causal_format(test_raw)\n",
    "print(\"Sample formatted:\", train_fmt[0][\"text\"][:200])\n",
    "\n",
    "tokenizer = load_tokenizer(CFG)\n",
    "\n",
    "tok_fn = tokenize_fn(tokenizer, CFG.max_length)\n",
    "train_tok = train_fmt.map(tok_fn, batched=True, remove_columns=train_fmt.column_names)\n",
    "test_tok  = test_fmt.map(tok_fn, batched=True, remove_columns=test_fmt.column_names)\n",
    "\n",
    "print(\"Train tokenized columns:\", train_tok.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def prepare_for_training(model, tokenizer=None):\n",
    "    # Disable KV cache when using gradient checkpointing\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False\n",
    "\n",
    "    # Ensure inputs can require grads (important when most weights are frozen with PEFT)\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "        model.get_input_embeddings().requires_grad_(True)\n",
    "\n",
    "    # Be explicit about PAD token id (prevents some generate() mishaps)\n",
    "    if tokenizer is not None and getattr(model.config, \"pad_token_id\", None) is None:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Sanity check: confirm we actually have trainable params\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.4f}% trainable)\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "/home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 294,912 / 124,737,024 (0.2364% trainable)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29199/2514287141.py:70: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 50258, 'pad_token_id': 50257}.\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4375' max='4375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4375/4375 43:14, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.559000</td>\n",
       "      <td>2.480579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.483200</td>\n",
       "      <td>2.401433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.456800</td>\n",
       "      <td>2.363074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.437500</td>\n",
       "      <td>2.341262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.408900</td>\n",
       "      <td>2.326287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.416500</td>\n",
       "      <td>2.313534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.406900</td>\n",
       "      <td>2.307347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.405300</td>\n",
       "      <td>2.302206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "EN: But suddenly she heard the rustle of a dress and a burst of suppressed sobbing. A pair of arms encircled her neck from below and Kitty was kneeling before her.\n",
      "REF: Но вдруг она услыхала шум платья и вместе звук разразившегося сдержанного рыданья, и чьи-то руки снизу обняли ее шею. Кити на коленях стояла пред ней.\n",
      "HYP: Она она она она она она она она она она она она она она она.\n",
      "------------------------------------------------------------\n",
      "EN: 'Yes, tell me what is happening in Pokrovsk Is the house still standing, and the birch trees, and our schoolroom?\n",
      "REF: -- Да расскажи мне, что делается в Покровском? Что, дом все стоит, и березы, и наша классная?\n",
      "HYP: -- Она, что что что что что что что что что что что что что что что что что что что что что что что что �\n",
      "------------------------------------------------------------\n",
      "EN: 'Matthew!' he called, 'will you and Mary arrange everything for Anna Arkadyevna in the little sitting-room?' he added when Matthew appeared.\n",
      "REF: -- Матвей!-- крикнул он, -- так устрой же все там с Марьей в диванной для Анны Аркадьевны, -- сказал он явившемуся Матвею.\n",
      "HYP: -- Она, -- сказала, -- сказала, -- сказала, -- сказала, -- сказала.\n",
      "\n",
      "[LoRA] Computing metrics...\n",
      "BLEU      : 0.44\n",
      "chrF      : 6.39\n",
      "ROUGE-1   : 0.33\n",
      "ROUGE-2   : 0.00\n",
      "ROUGE-L   : 0.33\n",
      "ROUGE-Lsum: 0.33\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 6) LoRA — train & evaluate (run this cell to do plain LoRA)\n",
    "# This cell attaches **LoRA adapters**, trains, prints a few samples, and reports **BLEU/chrF/ROUGE**.\n",
    "\n",
    "# %%\n",
    "# --- Base model (no quantization here) ---\n",
    "model_lora = load_base_model(CFG, tokenizer)\n",
    "\n",
    "# --- Attach LoRA ---\n",
    "lora_cfg = LoraConfig(\n",
    "    r=CFG.lora_r,\n",
    "    lora_alpha=CFG.lora_alpha,\n",
    "    target_modules=[\"c_attn\"],  # GPT-2 attention proj\n",
    "    lora_dropout=CFG.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "model_lora = get_peft_model(model_lora, lora_cfg)\n",
    "model_lora.gradient_checkpointing_enable()\n",
    "model_lora = prepare_for_training(model_lora, tokenizer)   # <-- add this line\n",
    "model_lora.to(DEVICE)\n",
    "\n",
    "# --- Train ---\n",
    "trainer_lora = build_trainer(model_lora, tokenizer, train_tok, test_tok, CFG, out_dir_suffix=\"lora\")\n",
    "trainer_lora.train()\n",
    "\n",
    "# --- Quick qualitative check ---\n",
    "for i in range(3):\n",
    "    ex = test_raw[i]\n",
    "    hyp = generate_translation(model_lora, tokenizer, ex[\"en\"], CFG)\n",
    "    print(\"-\"*60)\n",
    "    print(\"EN:\", ex[\"en\"])\n",
    "    print(\"REF:\", ex[\"ru\"])\n",
    "    print(\"HYP:\", hyp)\n",
    "\n",
    "# --- Metrics ---\n",
    "print(\"\\n[LoRA] Computing metrics...\")\n",
    "scores = compute_metrics_on_subset(model_lora, tokenizer, test_raw, CFG)\n",
    "for k, v in scores.items():\n",
    "    print(f\"{k:10s}: {v:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39793/2514287141.py:70: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 50258, 'pad_token_id': 50257}.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='875' max='875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [875/875 47:56, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.586400</td>\n",
       "      <td>2.459620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "EN: But suddenly she heard the rustle of a dress and a burst of suppressed sobbing. A pair of arms encircled her neck from below and Kitty was kneeling before her.\n",
      "REF: Но вдруг она услыхала шум платья и вместе звук разразившегося сдержанного рыданья, и чьи-то руки снизу обняли ее шею. Кити на коленях стояла пред ней.\n",
      "HYP: Привать не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не\n",
      "------------------------------------------------------------\n",
      "EN: 'Yes, tell me what is happening in Pokrovsk Is the house still standing, and the birch trees, and our schoolroom?\n",
      "REF: -- Да расскажи мне, что делается в Покровском? Что, дом все стоит, и березы, и наша классная?\n",
      "HYP: -- Она прошевал прошевал прошевал прошевал прошевал прошевал прошевал прошевал.\n",
      "------------------------------------------------------------\n",
      "EN: 'Matthew!' he called, 'will you and Mary arrange everything for Anna Arkadyevna in the little sitting-room?' he added when Matthew appeared.\n",
      "REF: -- Матвей!-- крикнул он, -- так устрой же все там с Марьей в диванной для Анны Аркадьевны, -- сказал он явившемуся Матвею.\n",
      "HYP: -- Она простивал не простивал не простивал не простивал не простивал не простивал не простивал.\n",
      "\n",
      "[QLoRA] Computing metrics...\n",
      "BLEU      : 0.09\n",
      "chrF      : 7.59\n",
      "ROUGE-1   : 0.00\n",
      "ROUGE-2   : 0.00\n",
      "ROUGE-L   : 0.00\n",
      "ROUGE-Lsum: 0.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 8) QLoRA — train & evaluate (run this cell for QLoRA)\n",
    "# 4-bit quantization with bitsandbytes.  \n",
    "# **Requires** a GPU + matching CUDA build of `torch` and `bitsandbytes`.\n",
    "\n",
    "# %%\n",
    "# --- BitsAndBytes quantization config ---\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=CFG.qlora_4bit,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=CFG.qlora_compute_dtype,\n",
    ")\n",
    "\n",
    "# --- Base model in 4-bit ---\n",
    "model_qlora = load_base_model(CFG, tokenizer, quantization_config=bnb_cfg)\n",
    "\n",
    "# --- Prepare & attach LoRA adapters for QLoRA ---\n",
    "model_qlora = prepare_model_for_kbit_training(model_qlora)\n",
    "qlora_lora_cfg = LoraConfig(\n",
    "    r=CFG.lora_r,\n",
    "    lora_alpha=CFG.lora_alpha,\n",
    "    target_modules=[\"c_attn\"],\n",
    "    lora_dropout=CFG.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "model_qlora = get_peft_model(model_qlora, qlora_lora_cfg)\n",
    "model_qlora.gradient_checkpointing_enable()\n",
    "# device_map=\"auto\" is set already; no explicit .to(DEVICE)\n",
    "\n",
    "# --- Train ---\n",
    "trainer_qlora = build_trainer(model_qlora, tokenizer, train_tok, test_tok, CFG, out_dir_suffix=\"qlora\")\n",
    "trainer_qlora.train()\n",
    "\n",
    "# --- Quick qualitative check ---\n",
    "for i in range(3):\n",
    "    ex = test_raw[i]\n",
    "    hyp = generate_translation(model_qlora, tokenizer, ex[\"en\"], CFG)\n",
    "    print(\"-\"*60)\n",
    "    print(\"EN:\", ex[\"en\"])\n",
    "    print(\"REF:\", ex[\"ru\"])\n",
    "    print(\"HYP:\", hyp)\n",
    "\n",
    "# --- Metrics ---\n",
    "print(\"\\n[QLoRA] Computing metrics...\")\n",
    "scores = compute_metrics_on_subset(model_qlora, tokenizer, test_raw, CFG)\n",
    "for k, v in scores.items():\n",
    "    print(f\"{k:10s}: {v:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
