{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Важно!** \n",
    "\n",
    "Домашнее задание состоит из нескольких задач, которые вам нужно решить.\n",
    "*   Баллы выставляются по принципу выполнено/невыполнено.\n",
    "*   За каждую выполненую задачу вы получаете баллы (количество баллов за задание указано в скобках).\n",
    "\n",
    "**Инструкция выполнения:** Выполните задания в этом же ноутбуке (места под решения **КАЖДОЙ** задачи обозначены как **#НАЧАЛО ВАШЕГО РЕШЕНИЯ** и **#КОНЕЦ ВАШЕГО РЕШЕНИЯ**)\n",
    "\n",
    "**Как отправить задание на проверку:** Вам необходимо сохранить ваше решение в данном блокноте и отправить итоговый **файл .IPYNB** в личном сообщении Telegram.\n",
    "\n",
    "# **Прежде чем проверять задания:**\n",
    "\n",
    "1. Перезапустите **ядро (restart the kernel)**: в меню, выбрать **Ядро (Kernel)**\n",
    "→ **Перезапустить (Restart)**\n",
    "2. Затем **Выполнить** **все ячейки (run all cells)**: в меню, выбрать **Ячейка (Cell)**\n",
    "→ **Запустить все (Run All)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание — LangChain и инференс\n",
    "\n",
    "Цель: перевести домашнее задание на использование LangChain. В задании — 4 задачи, в том числе про LCEL и Structured Output. Для каждой задачи дан стартовый код."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Задачи (кратко)\n",
    "\n",
    "1. Task 1 — Быстрый старт LangChain: Prompt + LLM\n",
    "2. Task 2 — Chains и составные сценарии\n",
    "3. Task 3 — LCEL: сценарий с несколькими шагами логики\n",
    "4. Task 4 — Structured Output: схемы и валидация\n",
    "\n",
    "Дальше следует подробное описание каждой задачи с критериями и стартовым кодом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-1.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-2.6.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-2.12.3-py3-none-any.whl.metadata (87 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.0.0 (from langchain)\n",
      "  Downloading langchain_core-1.0.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting langgraph<1.1.0,>=1.0.0 (from langchain)\n",
      "  Downloading langgraph-1.0.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.4 (from pydantic)\n",
      "  Downloading pydantic_core-2.41.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages (from pydantic) (4.15.0)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.0.0->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.0.0->langchain)\n",
      "  Downloading langsmith-0.4.37-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (6.0.2)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<2.0.0,>=1.0.0->langchain)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Downloading langgraph_checkpoint-3.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.0 (from langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Downloading langgraph_prebuilt-1.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.0->langchain) (3.6.0)\n",
      "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Downloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting httpx>=0.25.2 (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson>=3.10.1 (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Downloading orjson-3.11.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: requests>=2.0.0 in /home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (0.24.0)\n",
      "Collecting anyio (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: certifi in /home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (2025.10.5)\n",
      "Collecting httpcore==1.* (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (3.10)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Downloading jiter-0.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in /home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vladislav/miniconda3/envs/nlp/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.5.0)\n",
      "Downloading langchain-1.0.1-py3-none-any.whl (106 kB)\n",
      "Downloading pydantic-2.12.3-py3-none-any.whl (462 kB)\n",
      "Downloading pydantic_core-2.41.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m454.3 kB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-1.0.0-py3-none-any.whl (467 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langgraph-1.0.1-py3-none-any.whl (155 kB)\n",
      "Downloading langgraph_checkpoint-3.0.0-py3-none-any.whl (46 kB)\n",
      "Downloading langgraph_prebuilt-1.0.1-py3-none-any.whl (28 kB)\n",
      "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
      "Downloading langsmith-0.4.37-py3-none-any.whl (396 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading openai-2.6.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m360.0 kB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading orjson-3.11.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
      "Downloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, tenacity, sniffio, pydantic-core, ormsgpack, orjson, jsonpointer, jiter, h11, distro, annotated-types, requests-toolbelt, pydantic, jsonpatch, httpcore, anyio, httpx, openai, langsmith, langgraph-sdk, langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph, langchain\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/25\u001b[0m [langchain]25\u001b[0m [langchain]core]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 anyio-4.11.0 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jiter-0.11.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-1.0.1 langchain-core-1.0.0 langgraph-1.0.1 langgraph-checkpoint-3.0.0 langgraph-prebuilt-1.0.1 langgraph-sdk-0.2.9 langsmith-0.4.37 openai-2.6.0 orjson-3.11.3 ormsgpack-1.11.0 pydantic-2.12.3 pydantic-core-2.41.4 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 typing-inspection-0.4.2\n"
     ]
    }
   ],
   "source": [
    "# Установка зависимостей (выполните в среде развертывания один раз)\n",
    "!pip install langchain openai pydantic\n",
    "\n",
    "OPENROUTER_API_KEY =  \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1 — Quick LangChain prompt\n",
    "\n",
    "Цель: собрать минимальную LCEL-цепочку «prompt → LLM → парсер», которая возвращает краткий ответ модели на запрос \"Механизм внимания\".\n",
    "\n",
    "Acceptance criteria:\n",
    "- Используется `ChatPromptTemplate` и `ChatOpenAI` (или совместимая чат-модель), соединённые через LCEL (`|`).\n",
    "- В цепочку добавлен `StrOutputParser` и приведён пример вызова `chain.invoke(...)`.\n",
    "- В примере показано, где брать API-ключ (например, из переменной окружения `OPENROUTER_API_KEY`).\n",
    "\n",
    "Starter code:\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Ты отвечаешь кратко и по делу на русском языке.\"),\n",
    "    (\"human\", \"Объясни тему: {topic}\"),\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-5-nano\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\", OPENROUTER_API_KEY),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"topic\": \"LSTM\"})\n",
    "print(response)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ключевая идея: внимание — это способ сосредоточиться на наиболее значимой части входа и переработать её с большими ресурсами, игнорируя остальное.\n",
      "\n",
      "1) В контексте человеческого восприятия (когнитивная психология)\n",
      "- Избирательное внимание: ограниченные ресурсы памяти и обработки, поэтому мы выбираем, что смотреть и слышать.\n",
      "- Типы: топ-даун (цели/ожидания направляют внимание) и боттом-ап (сигналы среды привлекают внимание).\n",
      "- Важные свойства: фокус на релевантной информации, способность переключаться между объектами, подавление нерелевантного.\n",
      "\n",
      "2) Механизм внимания в машинном обучении (нейронные сети)\n",
      "- Что делает: модель присваивает входным элементам веса важности и фокусируется на самых информативных частях.\n",
      "- Основные виды:\n",
      "  - Soft attention: дифференцируемое взвешивание, можно обучать через градиенты.\n",
      "  - Hard attention: выбор конкретной части (часто требует стохастических методов).\n",
      "  - Self-attention: элементы внутри одного входа взаимодействуют друг с другом (важно в Transformer).\n",
      "  - Multi-head attention: несколько «голов» внимания, чтобы ловить разные зависимости.\n",
      "- Как работает (упрощённо, для последовательностей):\n",
      "  - Есть запросы Q, ключи K и значения V.\n",
      "  - Веса внимания: softmax(QK^T / sqrt(d_k)).\n",
      "  - Выход: сумма весов на V (weighted sum).\n",
      "- Применение: перевод текстов, обработка речи и звука, анализ изображений и видео (например, локализация важных областей).\n",
      "- Преимущества: гибкость для учёта долгосрочных зависимостей, лучшее распознавание важных частей входа, частично улучшенная интерпретируемость.\n",
      "- Ограничения: вычислительная сложность и память, требуются данные и регуляризация.\n",
      "\n",
      "Коротко: у людей внимание управляет тем, что мы обрабатываем глубже; в моделях внимания — весовая настройка входных частей, чтобы сеть фокусировалась на наиболее информативном. Хотите показать на простом примере или формуле подробнее?\n"
     ]
    }
   ],
   "source": [
    "# Task 1 — Quick LangChain prompt\n",
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "import os\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Ты отвечаешь кратко и по делу на русском языке.\"),\n",
    "    (\"human\", \"Объясни тему: {topic}\"),\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-5-nano\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\", OPENROUTER_API_KEY),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"topic\": \"Механизм внимания\"})\n",
    "print(response)\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 — Chains: составной pipeline\n",
    "\n",
    "Цель: собрать LCEL-пайплайн из двух шагов: (1) LLM подбирает факты по теме `\"transformers\"`, (2) отдельный шаг формирует резюме из заметок.\n",
    "\n",
    "Acceptance criteria:\n",
    "- Используется LangChain Expression Language (композиции `|`, `RunnablePassthrough`, `RunnableLambda`) вместо `SequentialChain`. `RunnablePassthrough` просто прокидывает вход дальше по цепочке без изменений. В LCEL его используют, когда нужно передать исходные данные как часть словаря в последующие шаги: например, `{\"topic\": RunnablePassthrough(), \"research_notes\": research_chain}` — так и оригинальная тема, и результаты ресёрча доступны в следующем промпте.\n",
    "- Шаблоны prompt'ов для ресёрча и итогового резюме разделены и связаны логикой.\n",
    "- Есть пример вызова `.invoke` c входным топиком.\n",
    "\n",
    "Starter code:\n",
    "\n",
    "```python\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-5-nano\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\", OPENROUTER_API_KEY),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "research_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Собери три факта по теме.\"),\n",
    "    (\"human\", \"Тема: {topic}\"),\n",
    "])\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Сделай короткое резюме на русском.\"),\n",
    "    (\"human\", \"Суммаризируй заметки: {research_notes}\"),\n",
    "])\n",
    "\n",
    "research_chain = research_prompt | llm | StrOutputParser()\n",
    "\n",
    "pipeline = (\n",
    "    {\n",
    "        \"topic\": RunnablePassthrough(),\n",
    "        \"research_notes\": research_chain,\n",
    "    }\n",
    "    | summary_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = pipeline.invoke(\"rnns\")\n",
    "print(response)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Короткое резюме:\n",
      "- Transformer заменил рекуррентные и сверточные слои механизмом self-attention, позволяет обрабатывать вход целиком и параллельно; обычно есть энкодер и/или декодер и позиционные кодирования для сохранения порядка слов.\n",
      "- Self-attention строит весовые коэффициенты между всеми парами токенов через Q, K, V; результат — взвешенная сумма значений; multi-head позволяет улавливать разные зависимости.\n",
      "- Transformer стал доминирующей архитектурой в NLP благодаря предобучению на больших корпусах с последующим fine-tuning; применяется также в vision (ViT), аудио и биоинформатике; требует больших данных и вычислений, что стимулирует развитие эффективных вариантов внимания (линейное/разреженное).\n"
     ]
    }
   ],
   "source": [
    "# Task 2 — Исследование + резюме в одном chain\n",
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-5-nano\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\", OPENROUTER_API_KEY),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "research_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Собери три факта по теме.\"),\n",
    "    (\"human\", \"Тема: {topic}\"),\n",
    "])\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Сделай короткое резюме на русском.\"),\n",
    "    (\"human\", \"Суммаризируй заметки: {research_notes}\"),\n",
    "])\n",
    "\n",
    "research_chain = research_prompt | llm | StrOutputParser()\n",
    "\n",
    "pipeline = (\n",
    "    {\n",
    "        \"topic\": RunnablePassthrough(),\n",
    "        \"research_notes\": research_chain,\n",
    "    }\n",
    "    | summary_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = pipeline.invoke(\"transformers\")\n",
    "print(response)\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 — LCEL: сложная логика в несколько шагов\n",
    "\n",
    "Цель: построить условный роутер запросов на LCEL с несколькими ветками и fallback.\n",
    "1 Дать определение диффузии и 2 сравнить RNN и Transformer\n",
    "\n",
    "Acceptance criteria:\n",
    "- Предусмотрена классификация интента (`RunnableLambda` добавляет ключ `intent`).\n",
    "- `RunnableBranch` маршрутизирует запрос в разные цепочки и содержит запасной путь.\n",
    "- Показан пример вызова `.invoke`, демонстрирующий выбранную ветку.\n",
    "\n",
    "Starter code / схема:\n",
    "\n",
    "```python\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch, RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-5-nano\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\", OPENROUTER_API_KEY),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "definition_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Дай определение: {question}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "comparison_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Сравни подходы: {question}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def classify(inputs):\n",
    "    question = inputs[\"question\"].lower()\n",
    "    if \"сравн\" in question:\n",
    "        intent = \"comparison\"\n",
    "    elif \"что такое\" in question or \"определ\" in question:\n",
    "        intent = \"definition\"\n",
    "    else:\n",
    "        intent = \"fallback\"\n",
    "    return {\"intent\": intent, **inputs}\n",
    "\n",
    "router = RunnableBranch(\n",
    "    (lambda data: data[\"intent\"] == \"comparison\", comparison_chain),\n",
    "    (lambda data: data[\"intent\"] == \"definition\", definition_chain),\n",
    "    RunnableLambda(lambda data: \"Не знаю, уточните запрос.\"),\n",
    ")\n",
    "\n",
    "workflow = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | RunnableLambda(classify)\n",
    "    | router\n",
    ")\n",
    "response1 = workflow.invoke(\"Что такое LSTM?\")\n",
    "\n",
    "\n",
    "print(response1)\n",
    "\n",
    "response2 = workflow.invoke(\"Сравни TF-IDF и BoW\")\n",
    "\n",
    "print(response2)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Диффузия — самопроизвольный процесс перемещения молекул или частиц вещества из области с более высокой концентрацией в область с более низкой, в результате которого концентрации стремятся стать равными (наблюдается выравнивание концентраций).\n",
      "\n",
      "Ключевые моменты:\n",
      "- механизм: тепловое (случайное) движение частиц и их столкновения.\n",
      "- характер: без внешних сил обычно; скорость зависит от температуры, вязкости среды и размера частиц.\n",
      "- в физике/химии часто описывается законом Фика: J = -D ∂C/∂x, где J — поток вещества, D — коэффициент диффузии, ∂C/∂x — градиент концентрации.\n",
      "- примеры: диффузия кислорода из легких в кровь, диффузия запаха по комнате, окрашивание воды краской.\n",
      "Ниже — краткое, практическое сравнение RNN (LSTM/GRU и их вариации) и Transformer.\n",
      "\n",
      "1) Принципиальный подход\n",
      "- RNN: обрабатывает последовательность шаг за шагом через рекуррентное состояние. Каждое новое скрытое состояние зависит от предыдущего.\n",
      "- Transformer: полная[self-attention] архитектура. Механизм самовнимания позволяет всем элементам последовательности взаимодействовать напрямую, без явной рекурсии. В обучении используется кодер/декодер (часто энкодер для входной последовательности и авто-регрессивный декодер для генерации).\n",
      "\n",
      "2) Архитектура и индуктивные смещения\n",
      "- RNN: сильный индуктивный bias по времени (последовательность \"как flowing рекурсивно во времени\"). Хорошо работает на локальных зависимостях и коротких/средних контекстах.\n",
      "- Transformer: слабее навязанный последовательный контракт, зато мощная способность моделировать дальние зависимости через слои самовнимания. Это требует позиционных кодировок (или альтернатив) для сохранения порядка.\n",
      "\n",
      "3) Вычислительная сложность и память\n",
      "- RNN: посекционный характер обучения ограничен последовательной обработкой; можно лучше использовать в ресурсах ограниченных ноутбуках. Сложность примерно O(T) по времени на один слой, но без реального параллелизма между шагами.\n",
      "- Transformer: обучение и инференс можно параллелизовать за счет независимых операций по позициям в каждом слое, что даёт большую скорость на больших датасетах и длинных последовательностях. Однако self-attention имеет квадратичную по длине сложность O(T^2) по памяти и времени, что становится проблемой на очень длинных контекстах. Существуют варианты (Longformer, Linformer, Reformer, Transformer-XL) для смягчения этой проблемы.\n",
      "\n",
      "4) Обучение и данные\n",
      "- RNN: часто стабильнее в меньших наборах данных; требует меньше вычислительных ресурсов в плане памяти на длинные последовательности; склонны к проблемам исчезающего/взрыва градиента, особенно без gated-ячейек (LSTM/GRU их снимают частично).\n",
      "- Transformer: лучше качество на больших датасетах и сложных задачах за счёт мощной зависимости на дальних контекстах; требует большего объема данных и вычислительных ресурсов для эффективного обучения. Может быть менее устойчив в малых данных без регуляризации/привязок.\n",
      "\n",
      "5) Генерация и инференс\n",
      "- RNN: инференс естественно пошаговый; генерация может быть медленной из-за последовательной природы, но в некоторых задачах хорошо подходит для онлайн/стриминга.\n",
      "- Transformer: во время обучения декодер наследует возможность параллельной подготовки входной части, но процесс генерации всё равно авто-regressive (по одному токену за шаг). Вводит эффективные техники ускорения декодирования и кэширования внимания.\n",
      "\n",
      "6) Применимость и типичные задачи\n",
      "- RNN:\n",
      "  - Классические задачи последовательной обработки: предварительная обработка времени, сигналов, небольшиеSeq2Seq задачи с ограниченным контекстом.\n",
      "  - Би-направленные варианты для кодирования контекста; подходят, когда важны локальные зависимости и онлайн-обработке нет строгих требований к длинному контексту.\n",
      "- Transformer:\n",
      "  - Большие тексты и языковые модели (перевод, резюмирование, генеративные задачи, кодогенерация, вопросно-ответные системы).\n",
      "  - Задачи, где важна долгосрочная зависимость и большая контекстная информация.\n",
      "  - Современные архитектуры: Transformer-Encoder/Decoder, обучающие большие языковые модели, архитектуры с улучшенной обработкой длинных контекстов (Longformer, Reformer, Transformer-XL и т.д.).\n",
      "\n",
      "7) Преимущества и ограничения\n",
      "- RNN:\n",
      "  - Преимущества: простота, меньше требований к данным, естественная работа со временем и переменной длиной последовательности.\n",
      "  - Ограничения: слабая способность к моделированию длинных зависимостей в vanilla RNN; медленная тренировка из-за последовательности шагов.\n",
      "- Transformer:\n",
      "  - Преимущества: очень сильное моделирование зависимостей на больших контекстах; хорошая параллелизация и масштабируемость на больших датасетах; гибкость в настройке слоев внимания и глубины.\n",
      "  - Ограничения: требует больших вычислительных ресурсов и данных; квадратичная по длине сложность внимания ограничивает длинные последовательности без специальных модификаций.\n",
      "\n",
      "8) Варианты и гибриды\n",
      "- Гибриды: можно сочетать RNN-слой с Transformer-слоем (например, предварительная обработка последовательности RNN перед вниманием, или вставка локальных RNN внутри Transformer-слоев).\n",
      "- Варианты Transformer: линейное внимание (Linformer), эффективное внимaние для длинных последовательностей (Longformer, BigBird), архитектуры с повторяемой памятью (Transformer-XL) и т. д.\n",
      "- Варианты RNN: LSTM, GRU, BiLSTM/BiGRU, Stateful RNN для длинных контекстов, многослойные RNN.\n",
      "\n",
      "9) Как выбрать между ними (практические guidelines)\n",
      "- Малые датасеты/ограниченные вычислительные ресурсы, короткие или средние последовательности: начинайте с RNN/LSTM/GRU.\n",
      "- Нужно работать с длинными контекстами, больших датасетов, или требуется высокая точность на языковых задачах: смотрите в сторону Transformer.\n",
      "- Требуется онлайн/стриминг-обработка: RNN может быть предпочтительнее, хотя подобрать Transformer-варианты для стриминга тоже возможно (с соответствующими модификациями внимания).\n",
      "- Если важна скорость обучения на больших данных: Transformer обычно выигрывает при достаточных ресурсах.\n",
      "\n",
      "Коротко подытожим:\n",
      "- RNN хороши для меньших задач, где важна простота и естественная обработка времени; ограничены зависимостями на дальних расстояниях и медленны в обучении.\n",
      "- Transformer — современная основа для большинства крупных задач NLP и последовательных задач с долгими контекстами, но требует больших вычислительных ресурсов и данных; с учётом модификаций хорошо масштабируются и дают лучший результат на больших задачах.\n",
      "\n",
      "Если хотите, могу привести конкретные примеры задач и соответствующие архитектуры (напр., для перевода, резюмирования, временных рядов и т. п.), а также подобрать рекомендуемую конфигурацию в зависимости от ваших ограничений по вычислениям и объему данных.\n"
     ]
    }
   ],
   "source": [
    "# Task 3 — LCEL: сложная логика в несколько шагов\n",
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch, RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-5-nano\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\", OPENROUTER_API_KEY),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "definition_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Дай определение: {question}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "comparison_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Сравни подходы: {question}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def classify(inputs):\n",
    "    question = inputs[\"question\"].lower()\n",
    "    if \"сравн\" in question:\n",
    "        intent = \"comparison\"\n",
    "    elif \"что такое\" in question or \"определ\" in question:\n",
    "        intent = \"definition\"\n",
    "    else:\n",
    "        intent = \"fallback\"\n",
    "    return {\"intent\": intent, **inputs}\n",
    "\n",
    "router = RunnableBranch(\n",
    "    (lambda data: data[\"intent\"] == \"comparison\", comparison_chain),\n",
    "    (lambda data: data[\"intent\"] == \"definition\", definition_chain),\n",
    "    RunnableLambda(lambda data: \"Не знаю, уточните запрос.\"),\n",
    ")\n",
    "\n",
    "workflow = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | RunnableLambda(classify)\n",
    "    | router\n",
    ")\n",
    "response1 = workflow.invoke(\"Что такое диффузия?\")\n",
    "\n",
    "\n",
    "print(response1)\n",
    "\n",
    "response2 = workflow.invoke(\"Сравни RNN и Transformer\")\n",
    "\n",
    "print(response2)\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 — Structured Output\n",
    "\n",
    "Цель: использовать актуальный `PydanticOutputParser` для строгой схемы ответа для темы `LLM`.\n",
    "\n",
    "Acceptance criteria:\n",
    "- Описана Pydantic-модель с типами и описаниями полей.\n",
    "- Prompt включает `parser.get_format_instructions()` и цепочка вызывает `.invoke`.\n",
    "- Полученный результат приводится к `dict`/`BaseModel` и выводится.\n",
    "\n",
    "Starter code:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class Fact(BaseModel):\n",
    "    title: str = Field(..., description=\"Короткий заголовок\")\n",
    "    summary: str\n",
    "    confidence: float\n",
    "\n",
    "class FactCollection(BaseModel):\n",
    "    topic: str\n",
    "    facts: list[Fact]\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=FactCollection)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Формируй JSON со списком фактов.\"),\n",
    "    (\"human\", \"Тема: {topic}\\nФормат: {format_instructions}\"),\n",
    "]).partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-5-nano\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\", OPENROUTER_API_KEY),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "structured_chain = prompt | llm | parser\n",
    "\n",
    "result = structured_chain.invoke({\"topic\": \"Attention\"})\n",
    "print(result)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic='LLM' facts=[Fact(title='Обучение на больших корпусах', summary='LLM обучаются на больших смешанных текстовых данных, что позволяет моделям захватывать широкий спектр языковых паттернов.', confidence=0.92), Fact(title='Пре-тренинг и настройка', summary='Предобучение на неразмеченных данных, затем настройка на инструкции (instruction tuning) и RLHF для повышения полезности.', confidence=0.95), Fact(title='Спектр задач и способностей', summary='Генерация текста, ответы на вопросы, перевод, суммирование и кодирование; фактологические ошибки возможны.', confidence=0.88), Fact(title='Ограничения и риски', summary='Стереотипы, вредный контент, зависимость от данных и ошибки в рассуждениях.', confidence=0.9), Fact(title='Этика и безопасность', summary='Необходимы политики использования, мониторинг вывода и фильтрация небезопасного контента.', confidence=0.87)]\n"
     ]
    }
   ],
   "source": [
    "# Task 4 — Structured Output\n",
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class Fact(BaseModel):\n",
    "    title: str = Field(..., description=\"Короткий заголовок\")\n",
    "    summary: str\n",
    "    confidence: float\n",
    "\n",
    "class FactCollection(BaseModel):\n",
    "    topic: str\n",
    "    facts: list[Fact]\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=FactCollection)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Формируй JSON со списком фактов.\"),\n",
    "    (\"human\", \"Тема: {topic}\\nФормат: {format_instructions}\"),\n",
    "]).partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-5-nano\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\", OPENROUTER_API_KEY),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "structured_chain = prompt | llm | parser\n",
    "\n",
    "result = structured_chain.invoke({\"topic\": \"LLM\"})\n",
    "print(result)\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
